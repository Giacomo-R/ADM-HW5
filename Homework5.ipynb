{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import datetime\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "\n",
    "import networkx as nx\n",
    "import queue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('wikigraph_reduced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>edge</th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>796</td>\n",
       "      <td>95</td>\n",
       "      <td>1185516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>909</td>\n",
       "      <td>108</td>\n",
       "      <td>1059989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>910</td>\n",
       "      <td>108</td>\n",
       "      <td>1062426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>911</td>\n",
       "      <td>108</td>\n",
       "      <td>1161925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1141</td>\n",
       "      <td>134</td>\n",
       "      <td>541222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1143</td>\n",
       "      <td>134</td>\n",
       "      <td>1061485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1146</td>\n",
       "      <td>134</td>\n",
       "      <td>1163610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1147</td>\n",
       "      <td>134</td>\n",
       "      <td>1163783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1309</td>\n",
       "      <td>153</td>\n",
       "      <td>744272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1311</td>\n",
       "      <td>153</td>\n",
       "      <td>1064807</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   edge source   target\n",
       "0   796     95  1185516\n",
       "1   909    108  1059989\n",
       "2   910    108  1062426\n",
       "3   911    108  1161925\n",
       "4  1141    134   541222\n",
       "5  1143    134  1061485\n",
       "6  1146    134  1163610\n",
       "7  1147    134  1163783\n",
       "8  1309    153   744272\n",
       "9  1311    153  1064807"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df['\\t0\\t1'].str.split('\\t', n=2, expand = True)\n",
    "df.rename(columns={0: \"edge\",1: \"source\", 2: \"target\"}, inplace = True)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = pd.read_csv('wiki-topcats-page-names.txt', header = None, names = ['page'])\n",
    "pages = pages['page'].str.split(' ', n=1, expand = True)\n",
    "pages.drop(0,axis=1, inplace = True)\n",
    "pages.rename(columns={1:'pageName'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pageName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chiasmal syndrome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kleroterion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pinakion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LyndonHochschildSerre spectral sequence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Zariski's main theorem</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  pageName\n",
       "0                        Chiasmal syndrome\n",
       "1                              Kleroterion\n",
       "2                                 Pinakion\n",
       "3  LyndonHochschildSerre spectral sequence\n",
       "4                   Zariski's main theorem"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file = open(\"wiki-topcats-categories.txt\", \"r\")\n",
    "contents = file.read()\n",
    "contents = contents.split(\"\\n\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Anlysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17365/17365 [00:00<00:00, 29200.63it/s]\n"
     ]
    }
   ],
   "source": [
    "#dictionary that has for keys the category and for value the list of pages in that category\n",
    "dictionary_cat = {}\n",
    "\n",
    "for i in tqdm(range(len(contents))):\n",
    "    \n",
    "    temp = contents[i].split(\";\")    \n",
    "    if len(temp)>1:\n",
    "    \n",
    "        key = temp[0]\n",
    "        key = key[9::]\n",
    "        key\n",
    "\n",
    "        list_page_frcat = temp[1].split(\" \")\n",
    "        list_page_frcat = list_page_frcat[1::]\n",
    "        list_page_frcat\n",
    "\n",
    "        dictionary_cat[key] = list_page_frcat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17364"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this dictionary we need to remove first the categories that have less than 5000 and more than 30000 pages associated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_to_drop = []\n",
    "for key, value in dictionary_cat.items():\n",
    "    if len(value)<5000 or len(value)>30000:\n",
    "        categories_to_drop.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17343"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(categories_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new dictionary by including only the categories that are not in the list categories_to_drop\n",
    "dictionary_cat = {i:dictionary_cat[i] for i in dictionary_cat if i not in categories_to_drop}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['English_footballers', 'The_Football_League_players', 'Association_football_forwards', 'Association_football_midfielders', 'Association_football_defenders', 'Harvard_University_alumni', 'Major_League_Baseball_pitchers', 'Members_of_the_United_Kingdom_Parliament_for_English_constituencies', 'Indian_films', 'Year_of_death_missing', 'Rivers_of_Romania', 'Main_Belt_asteroids', 'Asteroids_named_for_people', 'English-language_films', 'American_films', 'American_television_actors', 'American_film_actors', 'Debut_albums', 'Black-and-white_films', 'Year_of_birth_missing', 'Place_of_birth_missing_(living_people)'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary_cat.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English_footballers: 9237\n",
      "The_Football_League_players: 9467\n",
      "Association_football_forwards: 6959\n",
      "Association_football_midfielders: 8270\n",
      "Association_football_defenders: 6668\n",
      "Harvard_University_alumni: 6154\n",
      "Major_League_Baseball_pitchers: 6580\n",
      "Members_of_the_United_Kingdom_Parliament_for_English_constituencies: 6546\n",
      "Indian_films: 5913\n",
      "Year_of_death_missing: 7851\n",
      "Rivers_of_Romania: 7729\n",
      "Main_Belt_asteroids: 13704\n",
      "Asteroids_named_for_people: 5701\n",
      "English-language_films: 22699\n",
      "American_films: 15302\n",
      "American_television_actors: 11661\n",
      "American_film_actors: 13938\n",
      "Debut_albums: 8401\n",
      "Black-and-white_films: 12174\n",
      "Year_of_birth_missing: 7237\n",
      "Place_of_birth_missing_(living_people): 6767\n"
     ]
    }
   ],
   "source": [
    "for key,value in dictionary_cat.items():\n",
    "    print(f'{key}: {len(value)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:00<00:00, 179.77it/s]\n"
     ]
    }
   ],
   "source": [
    "#dictionary that associates to each page, the list of categories that is in\n",
    "inverted_link = {}\n",
    "\n",
    "for key,value in tqdm(dictionary_cat.items()):\n",
    "    if value != None:\n",
    "\n",
    "        for elem in value: \n",
    "            if elem not in inverted_link.keys():             \n",
    "                inverted_link[elem] = []\n",
    "                inverted_link[elem].append(key)\n",
    "\n",
    "            else:\n",
    "                inverted_link[elem].append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of pages that appear in more than one category: 41555\n"
     ]
    }
   ],
   "source": [
    "#list that has all the pages that have are associated to more than 1 category\n",
    "key_list = []\n",
    "for key in inverted_link.keys():\n",
    "    if len(inverted_link[key])>1:\n",
    "        key_list.append(key)\n",
    "print(f'number of pages that appear in more than one category: {len(key_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['English_footballers', 'Association_football_defenders']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverted_link[\"22860\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary that shows the distribution of the pages in each category\n",
    "len_list = {}\n",
    "\n",
    "for key in key_list:     \n",
    "    lenght = len(inverted_link[key]) \n",
    "    \n",
    "    if lenght not in len_list.keys():             \n",
    "                len_list[lenght] = 1\n",
    "    else:\n",
    "        len_list[lenght] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: 34371, 3: 6783, 4: 378, 5: 22, 6: 1}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(len_list.keys(), len_list.values(), width = 2.0, color='royalblue')\n",
    "plt.xlim = (0, max(len_list.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For every page in the inverted index (key) I uniformly choose at random the category in which belongs and I create a new dictionary where the categories are the keys and the pages in each category are the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149794/149794 [00:02<00:00, 53153.96it/s]\n"
     ]
    }
   ],
   "source": [
    "#final dictionary where the pages that appear in multiple categories are assigned to only one uniformly at random\n",
    "final_category_dict = dict.fromkeys(dictionary_cat.keys())\n",
    "\n",
    "for key,value in tqdm(inverted_link.items()):\n",
    "    category = str(*np.random.choice(value, size=1, replace=False, p= None))\n",
    "    if final_category_dict[category] == None:\n",
    "        final_category_dict[category] = [key]\n",
    "    else:\n",
    "        final_category_dict[category].append(key)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trial to see if the above operation worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:00<00:00, 49.83it/s]\n"
     ]
    }
   ],
   "source": [
    "#dictionary that associates to each page, the list of categories that it is in\n",
    "inverted_link2 = {}\n",
    "\n",
    "for key,value in tqdm(final_category_dict.items()):\n",
    "    if value != None:\n",
    "\n",
    "        for elem in value: \n",
    "            if elem not in inverted_link2.keys():             \n",
    "                inverted_link2[elem] = []\n",
    "                inverted_link2[elem].append(key)\n",
    "\n",
    "            else:\n",
    "                inverted_link2[elem].append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_list = []\n",
    "for key in inverted_link2.keys():\n",
    "    if len(inverted_link2[key])>1:\n",
    "        key_list.append(key)\n",
    "key_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This proves that now there are no pages that appear in multiple categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"l1 = [*df['target'].unique()]  #unique values from column target\\nl2 = [*df['source'].unique()]  #unique values from column source\\nl3 = list(set([*l1, *l2]))     #unique nodes to consider in the graph\\nlen(l3)\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''l1 = [*df['target'].unique()]  #unique values from column target\n",
    "l2 = [*df['source'].unique()]  #unique values from column source\n",
    "l3 = list(set([*l1, *l2]))     #unique nodes to consider in the graph\n",
    "len(l3)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'G = nx.MultiDiGraph()\\nG.add_nodes_from(l3)\\nG.add_weighted_edges_from([df.iloc[i] for i in range(len(df))],weight = 1 )'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''G = nx.MultiDiGraph()\n",
    "G.add_nodes_from(l3)\n",
    "G.add_weighted_edges_from([df.iloc[i] for i in range(len(df))],weight = 1 )'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the graph\n",
    "G = nx.from_pandas_edgelist(df, \"source\", \"target\",create_using=nx.MultiDiGraph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Is this the correct graph?* \n",
    "\n",
    "We can visualize all the edges of the graph by the command `G.edges()`, however since this will generate a long list we just check some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "edge      688634\n",
       "source     41914\n",
       "target    633362\n",
       "Name: 900, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[900]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdjacencyView({'633362': {0: {}}})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G['41914']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge      1193155\n",
      "source      65522\n",
      "target      35186\n",
      "Name: 1456, dtype: object \n",
      "\n",
      "edge      1193161\n",
      "source      65522\n",
      "target      64932\n",
      "Name: 1457, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.iloc[1456],'\\n')\n",
    "print(df.iloc[1457])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdjacencyView({'35186': {0: {}}, '64932': {0: {}}, '65074': {0: {}}})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G['65522'] # this node has multiple target values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the section **General Notes** on point 2. we read: 'We assume that all edges in the graphs we will consider have weight equal to 1'. Therefore we add this weight to each edge of the graph. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'AtlasView' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-83c8b121bae7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'AtlasView' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "for e in G.edges():\n",
    "    G[e[0]][e[1]]['weight'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answering the questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Is the graph directed?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, since the edges that compose the graph are ordered pairs of the form $(a,b)$. These pairs represent a hyperlink *from* the article $a$ *to* the article $b$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>find example of couple doesn't have reverse</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How many articles are we considering?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every node in the graph represents a wikipedia article. Therefore: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98343"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(G) # this returns the number of nodes in the graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How many hyperlinks between pages exist?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every edge in the graph represents a hyperlink between articles. Therefore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "483094"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.number_of_edges() # this returns the number of edges in the graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Notice that in counting hyperlinks the order of the pairs is important: $(a,b)$ and $(b,a)$ will count as $2$ hyperlinks.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute the average number of links in an arbitrary page. What is the graph density? Do you believe that the graph is dense or sparse? Is the graph dense?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first part of the question, the answer is given by the formula: $\\frac{|E|}{|V|}$ \n",
    "(where $|E|$ is the number of edges and $|V|$ is the number of vertices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.912337431235573"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.number_of_edges() / len(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second part of the question is important to understand what we mean by *graph density*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define *graph density* as the ratio between the number of edges and the maximum number of possible edges. In order to calculate what is the maximum number of edges of a graph, we need to understand what type of graph we are dealing with. \n",
    "`G` is a directed multigraph, therefore the set of all edges is $U := \\{(x,y) \\, \\,  | \\, \\, (x,y) \\in V^2: x \\neq y\\}$, therefore: $|U| = |V| \\cdot |V-1|$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The density of the graph `G` is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.9951571365597335e-05"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.number_of_edges() / ( len(G)*(len(G)-1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>This might be parse?</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize the nodes' degree distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_k histograms "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some final cleaning\n",
    "We need to clean one last time the categories file: we should consider only the nodes given in the reduced version of the graph. Therefore we are checking and removing the other ones and based on that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98343"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes_to_consider=set(G.nodes())\n",
    "len(nodes_to_consider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5070"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_category_dict['English_footballers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_to_drop = []\n",
    "for key, value in final_category_dict.items():\n",
    "    final_category_dict[key] = [values for values in value if values in nodes_to_consider]\n",
    "    if len(final_category_dict[key])<5000:\n",
    "        categories_to_drop.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3610"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([values for values in final_category_dict['English_footballers'] if values in nodes_to_consider])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English_footballers: 3610\n",
      "The_Football_League_players: 3616\n",
      "Association_football_forwards: 2525\n",
      "Association_football_midfielders: 2771\n",
      "Association_football_defenders: 2115\n",
      "Harvard_University_alumni: 2970\n",
      "Major_League_Baseball_pitchers: 3307\n",
      "Members_of_the_United_Kingdom_Parliament_for_English_constituencies: 6431\n",
      "Indian_films: 2742\n",
      "Year_of_death_missing: 1398\n",
      "Rivers_of_Romania: 7662\n",
      "Main_Belt_asteroids: 8697\n",
      "Asteroids_named_for_people: 2301\n",
      "English-language_films: 13292\n",
      "American_films: 7525\n",
      "American_television_actors: 6414\n",
      "American_film_actors: 9161\n",
      "Debut_albums: 2553\n",
      "Black-and-white_films: 5252\n",
      "Year_of_birth_missing: 1472\n",
      "Place_of_birth_missing_(living_people): 2529\n"
     ]
    }
   ],
   "source": [
    "for key,value in final_category_dict.items():\n",
    "    print(f'{key}: {len(value)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RQ2. Function 1\n",
    "\n",
    "Define a function that takes in input:\n",
    "\n",
    "- A page *v*\n",
    "- A number of clicks *d*\n",
    "\n",
    "and returns the set of all pages that a user can reach within *d* clicks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RQ3. Function 2\n",
    "\n",
    "Define a function that takes in input:\n",
    "\n",
    "- A category *C*\n",
    "- A set of pages in *C*, *p = {p1, ..., pn}*\n",
    "\n",
    "and returns the minimum number of clicks required to reach all pages in *p*, starting from the page *v*, corresponding to the most central article, according to the *in-degree* centrality, in *C*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Our interpretation of the problem consists of finding the maximum number of clicks needed to get from the selected central page v, to the furthest page away in terms of clicks. To solve this problem we worked with the neighbors of each page in the graph.\n",
    "\n",
    "Starting from page v our idea was to look at all the neighbors of v in the directed graph. From the list of neighbors generated we distinguished from the pages that belonged to the category in input and those who didn't. The first ones were all the pages that could be reached in 1 click from v, the second ones were stored because they could create a path to reach other pages in the category.\n",
    "\n",
    "In the second iteration we looked at the neighbors of all the pages found in iteration 1. Again we divided the pages into those who belonged to the category (that are now all the pages that can be reached in 2 clicks starting from v), and those who didn't.\n",
    "\n",
    "We repeated the process in a recursive way until the function returned an empty list of pages inside the category reached in that iteration. At this point, if the total number of pages of the category reached by our function was equal to the effective number of pages in the category, the function would return the number of iteration where it stopped, that is exactly the minimun number of clicks needed to reacha all pages $p$, starting from page $v$. If, on the other hand, the total was smaller, the function would just return 'NOT POSSIBLE' because it didn't find any path to get from page $v$ to all the pages of the category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in-degree(v): number of edges entering in v\n",
    "\n",
    "def most_central_article(df, pages, final_category_dict, category):\n",
    "    set_pages = final_category_dict[category] #set of pages corresponding to the category in input\n",
    "    \n",
    "    in_degrees = []\n",
    "    for page in set_pages:  #for every page in set_pages\n",
    "        \n",
    "        #counts how many links are pointing to that page \n",
    "        in_degrees.append(df.loc[df['target']==page]['edge'].count())\n",
    "    \n",
    "    most_central = set_pages[in_degrees.index(max(in_degrees))]\n",
    "    \n",
    "    return most_central"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#already implemented \n",
    "category = category = input('choose a category: ')\n",
    "\n",
    "subG = G.subgraph(final_category_dict[category])\n",
    "in_degrees = nx.in_degree_centrality(subG)\n",
    "\n",
    "m = max(in_degrees.values())\n",
    "for key, value in in_degrees.items():\n",
    "    if value == m:\n",
    "        v = key\n",
    "\n",
    "print(f'The starting node is: {v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#our implementation\n",
    "category = input('choose a category: ')\n",
    "v = most_central_article(df, pages, final_category_dict, category)\n",
    "print(f'The starting node is: {v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterations(set_pages, G, pages_reached, tot_pages_reached):\n",
    "    set_pages = list(list(set(set_pages)-set(pages_reached)) + list(set(pages_reached)-set(set_pages)))\n",
    "    pages_reached1 = []\n",
    "    for p in pages_reached:\n",
    "        try:\n",
    "            pages_reached1.append([n for n in G.neighbors(p) if n in set_pages and n not in tot_pages_reached])\n",
    "        except nx.exception.NetworkXError:\n",
    "            pass\n",
    "    \n",
    "    pages_reached1 = set(list(itertools.chain.from_iterable(pages_reached1)))\n",
    "    tot_pages_reached += list(pages_reached1)\n",
    "    \n",
    "    return pages_reached1, tot_pages_reached\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_clicks(G, df, pages, final_category_dict,v,category):\n",
    "    i = 0 \n",
    "    set_pages = final_category_dict[category]  \n",
    "    pages_reached = [n for n in G.neighbors(v) if n in set_pages]\n",
    "    tot_pages_reached = pages_reached\n",
    "    \n",
    "    print(f'Iteration {i}. Pages reached in {i} click: {len(pages_reached)}')\n",
    "    \n",
    "    while len(pages_reached) != 0:\n",
    "        i += 1\n",
    "        pages_reached, tot_pages_reached = iterations(set_pages, G, pages_reached, tot_pages_reached)\n",
    "        print(f'Iteration {i}. Pages reached in {i} click: {len(pages_reached)}')\n",
    "        \n",
    "    if len(tot_pages_reached) == len(final_category_dict[category]):\n",
    "        return f'minimum number of clicks required to reach all pages: {i+1}'\n",
    "    \n",
    "    else:\n",
    "        return i, f'Not Possible, total pages reached: {len(tot_pages_reached)} out of {len(final_category_dict[category])}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(min_clicks(G, df, pages, final_category_dict,v,category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for category in final_category_dict.keys():\n",
    "    print(category)\n",
    "    v = most_central_article(df, pages, final_category_dict, category)\n",
    "    print(min_clicks(G, df, pages, final_category_dict,v,category))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Different version that considers all the pages as neighbours, not only those pages that are in the list of pages of the category in input.\n",
    "\n",
    "\n",
    "Il metodo è molto molto più lento e in realtà la condizione per stoppare il while non è corretta perchè potrebbe essere che in una iterazione non trova nessuna pagina della categoria, ma dalle pagine trovate poi si ricollega all'iterazione successiva ad una pagina della categoria. In questo caso il sottografo indotto dalle pagine della cateogria non è connesso ma le pagine sono lo stesso raggiungibili attraverso nodi intermedi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This function checks, every time it is recalled, the neighbors of the page in input. It also differentiate between \n",
    "the pages that have to be reached, that are in the list of pages of the category in input, and the pages that \n",
    "are useful only to create a path but are not in the category of interest.\n",
    "In output returns:\n",
    "- the pages in the category of interest reached in that iteration\n",
    "- the pages not in the category of interest reached in that iteration\n",
    "- the updated list of pages visited, both from the category and not\n",
    "- the updated list of pages visited ONLY of the category of interest'''\n",
    "\n",
    "def iterations(set_pages, G, pages_reached_in_setpages, pages_reached_not_in_set, tot_pages_reached, tot_pages_only_set):\n",
    "    set_pages = list(list(set(set_pages)-set(pages_reached_in_setpages)) + list(set(pages_reached_in_setpages)-set(set_pages)))\n",
    "    pages_reached_in_setpages1 = []   #neighbors that are in the set of pages to reach\n",
    "    pages_reached_not_in_set1 = []    #all other neighbors\n",
    "    \n",
    "    tot = pages_reached_in_setpages +  pages_reached_not_in_set\n",
    "    #for every page find the list of neighbors and differentiate between those in the category and those who are not\n",
    "    for p in tot:\n",
    "        neighbors = [n for n in G.neighbors(p)]\n",
    "        try:\n",
    "            pages_reached_in_setpages1.append([n for n in neighbors if n in set_pages and n not in tot_pages_reached])\n",
    "            pages_reached_not_in_set1.append([n for n in neighbors if n not in set_pages and n not in tot_pages_reached])\n",
    "        \n",
    "        except nx.exception.NetworkXError:\n",
    "            pass\n",
    "    \n",
    "    #combine all neighbors of all pages into one list\n",
    "    pages_reached_in_setpages1 = list(set(list(itertools.chain.from_iterable(pages_reached_in_setpages1))))\n",
    "    pages_reached_not_in_set1 = list(set(list(itertools.chain.from_iterable(pages_reached_not_in_set1))))\n",
    "    \n",
    "    #update the list of visited pages\n",
    "    tot_pages_reached += pages_reached_in_setpages1 + pages_reached_not_in_set1\n",
    "    tot_pages_only_set += pages_reached_in_setpages1\n",
    "    \n",
    "    return pages_reached_in_setpages1, pages_reached_not_in_set1, tot_pages_reached, tot_pages_only_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Function that takes in input:\n",
    "- the graph G\n",
    "- the original dataframe with the information about the edges df\n",
    "- the dictionary that has for keys the category and for values the list of pages in that category\n",
    "- the starting page v\n",
    "- the cateogry chosen in input by the user\n",
    "\n",
    "The output is the minimum number of clicks needed to get from the central page v to the page that is further away from it\n",
    "- I have to click to the max number of links to get there. If I cannot get to ALL the pages in the category from my initial\n",
    "page, the algorithm stops and return the max number of pages it got before it had to stop'''\n",
    "\n",
    "def min_clicks(G, df, pages, final_category_dict,v,category):\n",
    "    i = 0 \n",
    "    set_pages = final_category_dict[category]                                           #set of all pages from the category in input\n",
    "    \n",
    "    neighbors = [n for n in G.neighbors(v)]                                             #all the neighbors of node v\n",
    "    pages_reached_in_setpages = [n for n in neighbors if n in set_pages]                #neighbors that are in the set of pages to reach\n",
    "    pages_reached_not_in_set = list(set(neighbors) - set(pages_reached_in_setpages))    #all other neighbors\n",
    "    \n",
    "    tot_pages_reached = pages_reached_in_setpages + pages_reached_not_in_set            #tot pages already reached \n",
    "    tot_pages_only_set = pages_reached_in_setpages                                      #pages inside the category already reached\n",
    "    \n",
    "    print(f'Iteration {i}. Pages reached in {i+1} click: {len(pages_reached_in_setpages)}')\n",
    "    \n",
    "    while len(pages_reached_in_setpages) != 0:\n",
    "        i += 1\n",
    "        pages_reached_in_setpages, pages_reached_not_in_set, tot_pages_reached, tot_pages_only_set = iterations(set_pages, G, pages_reached_in_setpages, \n",
    "                                                                                                                pages_reached_not_in_set, tot_pages_reached,\n",
    "                                                                                                                tot_pages_only_set)\n",
    "        print(f'Iteration {i}. Pages reached in {i+1} click: {len(pages_reached_in_setpages)}')\n",
    "        \n",
    "    if len(tot_pages_reached) == len(final_category_dict[category]):                    #if I visit all the pages in the category\n",
    "        return f'minimum number of clicks required to reach all pages: {i+1}'           #I return the maximum number of clicks to reach the page that is further than v\n",
    "    \n",
    "    #if the sum is not equal it means that there is at least one page that cannot be reached from v\n",
    "    else:\n",
    "        return i, f'Not Possible, total pages reached: {len(tot_pages_only_set)} out of {len(final_category_dict[category])}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in-degree built-in \n",
    "category = category = input('choose a category: ')\n",
    "\n",
    "subG = G.subgraph(final_category_dict[category])\n",
    "in_degrees = nx.in_degree_centrality(subG)\n",
    "\n",
    "m = max(in_degrees.values())\n",
    "for key, value in in_degrees.items():\n",
    "    if value == m:\n",
    "        v = key\n",
    "\n",
    "print(f'The starting node is: {v} \\n')\n",
    "print(min_clicks(G, df, pages, final_category_dict,v,category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in final_category_dict.keys():\n",
    "    print(category)\n",
    "    \n",
    "    subG = G.subgraph(final_category_dict[category])\n",
    "    in_degrees = nx.in_degree_centrality(subG)\n",
    "\n",
    "    m = max(in_degrees.values())\n",
    "    for key, value in in_degrees.items():\n",
    "        if value == m:\n",
    "            v = key\n",
    "\n",
    "    print(min_clicks(G, df, pages, final_category_dict,v,category))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RQ4. Induced Subgraph\n",
    "\n",
    "Given in input two categories: C1 and C2, we get the subgraph induced by all the articles in the two categories.\n",
    "\n",
    "- Let *v* and *u* two arbitrary pages in the subgraph. What is the minimum set of hyperlinks one can remove to disconnect *u* and *v*?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RQ5. Function 3\n",
    "\n",
    "Write a function that, given an arbitrary category C0 as input, returns the list of remaning categories sorted by their distance from C0. In particular, the distance between two categories is defined as  $distance(C0, Ci) = median(ShortestPath(C0, Ci))$ \n",
    "\n",
    "Where $ShortestPath(C0, Ci)$ is the set of shortest paths from each pair of nodes in the two categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RQ6. Function 4\n",
    "\n",
    "Write a function that sorts the categories in the graph according to their PageRank (PR). For this task you need to model the network of categories such that you can apply the PR algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the pagerank of a category as the sum of the pageranks of all the pages in that category, and the final output will return the category ordered in descending order based on this score.\n",
    "In order to apply the pagerank algorithm we need to assing to each edge a weight, which corresponds to the probability that a random surfer will follow a specific link starting from a random page. To assign these probabilities we need to look at each node, and from it, at all the edges exiting that node. The weight associated to the link going from node $i$ to node $j$ will then just be $w_{ij}  = \\frac{1}{\\textrm{#outlinks}(i)}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first thing we are going to define a new dictionary `outlinks_per_node` that has for keys the nodes names and for values the number of outlinks that the node has:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 98343/98343 [00:00<00:00, 569595.34it/s]\n"
     ]
    }
   ],
   "source": [
    "outlinks_per_node = defaultdict()\n",
    "for node in tqdm(G.nodes()):\n",
    "    outlinks_per_node[node] = len(G[node])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary that has for keys the nodes names and for values the number of incoming links in that node\n",
    "'''inlinks_per_node = defaultdict()\n",
    "indegrees = G.in_degree(G.nodes()) #list of tuples with 1st element id of the node and 2nd element in-degree\n",
    "for tup in indegrees:\n",
    "    inlinks_per_node[tup[0]] = tup[1]''' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will use it to assign a weight to each edge, using the formula described above. The list of weight will then be added as a new column in the original dataframe `df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "483094it [00:36, 13138.74it/s]\n"
     ]
    }
   ],
   "source": [
    "weight = []\n",
    "for row in tqdm(df.iterrows()):\n",
    "    weight.append(round(1/outlinks_per_node[row[1][1]], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>edge</th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>796</td>\n",
       "      <td>95</td>\n",
       "      <td>1185516</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>909</td>\n",
       "      <td>108</td>\n",
       "      <td>1059989</td>\n",
       "      <td>0.333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>910</td>\n",
       "      <td>108</td>\n",
       "      <td>1062426</td>\n",
       "      <td>0.333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>911</td>\n",
       "      <td>108</td>\n",
       "      <td>1161925</td>\n",
       "      <td>0.333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1141</td>\n",
       "      <td>134</td>\n",
       "      <td>541222</td>\n",
       "      <td>0.250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   edge source   target  weight\n",
       "0   796     95  1185516   1.000\n",
       "1   909    108  1059989   0.333\n",
       "2   910    108  1062426   0.333\n",
       "3   911    108  1161925   0.333\n",
       "4  1141    134   541222   0.250"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#I create a new dataframe from df that has also the weight column\n",
    "df1 = df.copy()\n",
    "df1['weight'] = weight\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new dataframe will be used to generate a new multidirected graph that assignes to each edge an attribute, given by the column **weight** of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "G1 = nx.from_pandas_edgelist(df1, \"source\", \"target\",create_using=nx.MultiDiGraph(), edge_attr = 'weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will implement the algorithm for the pagerank. To do so, we need to define a series a functions. The first step is to create the transition matrix where each row and each column refers to a node. The values in the matrix correspons to the probability to go from one node to the other, given by the weight that we have already computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "98343it [07:57, 205.86it/s] \n"
     ]
    }
   ],
   "source": [
    "M = np.zeros((len(G1.nodes()), len(G1.nodes())))  #initialize a matrix nxn (n = #nodes)\n",
    "nodes = [node for node in G1.nodes()]             #list of nodes in our graph\n",
    "\n",
    "#for each row that corresponds to each node\n",
    "for i,source in tqdm(enumerate(G1.nodes())):\n",
    "    nodes_linked = [elem for elem in G1[source]]   #list of nodes that can be reached from that node\n",
    "    for target in nodes_linked:\n",
    "        M[i][nodes.index(target)] = G1[source][target][0]['weight']  #I assign the weight that I previously calculated\n",
    "                                                                     #in the correct cell of the matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the values are probabilities, each row must sum to 1. There is one case where this is not true: when there are no outgoings link from one node. In this case, all the values of the corresponding row are going to be 0. To fix this case we will substitute each zero to $\\frac{1}{n}$ where $n$ is the number of nodes in the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7423\n"
     ]
    }
   ],
   "source": [
    "#to detect the rows of the matrix that sum to 0 instead of 1 we can use the dictionary that we have created before,\n",
    "#the row will sum to zero only if there are no outgoing links from it\n",
    "count = 0 \n",
    "zero_outlinks = []\n",
    "for key, value in outlinks_per_node.items():\n",
    "    if value == 0:\n",
    "        count +=1\n",
    "        zero_outlinks.append(str(key))\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7423/7423 [00:15<00:00, 485.50it/s] \n"
     ]
    }
   ],
   "source": [
    "#now we will change only the rows of the matrix that sum to 0 to sum = 1\n",
    "n = len(nodes)\n",
    "for elem in tqdm(zero_outlinks):\n",
    "    M[nodes.index(elem)] = 1/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pagerank(n_iter, a, nodes, M):\n",
    "    n = len(M)               #number of nodes\n",
    "    m_id = np.ones((n, n))   #matrix n*n with all ones \n",
    "\n",
    "    P = (1/n)*a*m_id + (1-a)*M              #probability to get to one node to the other\n",
    " \n",
    "    starting_page = random.choice(nodes)    #select a random page where to start random\n",
    "    v = np.zeros((1,len(nodes)))         \n",
    "    v[0][nodes.index(starting_page)] = 1.0  #create a strating vector that has 1 only in the position of the page selected\n",
    "\n",
    "    for i in range(n_iter):                 #repeat the process 100 time \n",
    "        v = v @ P\n",
    "        \n",
    "    return v                                #the final vector v will contain the probbaility of being at page i\n",
    "                                            #after all the iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_categories(v, nodes, final_category_dict):\n",
    "    pagerank = defaultdict()\n",
    "    for category, list_of_pages in final_category_dict.items():\n",
    "        pagerank[category] = sum([v[nodes.index[page]] for page in list_of_pages])\n",
    "        \n",
    "    sorted_dict = {key: v for key, value in sorted(pagerank.items(), key=lambda item: item[1])}\n",
    "    \n",
    "    return sorted_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "v = pagerank(100, 0.1, nodes, M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "for elem in v[0]:\n",
    "    c+=elem\n",
    "c   #perchè non fa 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_categories(v, nodes, final_category_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prova con le prime 1000 righe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.iloc[:1000].copy()\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n1 = list(df2.source.unique())\n",
    "n2 = list(df2.target.unique())\n",
    "nodes = list(set(n1 + n2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G2 = nx.from_pandas_edgelist(df2, \"source\", \"target\",create_using=nx.MultiDiGraph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outlinks_per_node2 = defaultdict()\n",
    "for node in tqdm(nodes):\n",
    "    outlinks_per_node2[node] = len(G2[node])\n",
    "    \n",
    "weight2 = []\n",
    "for row in tqdm(df2.iterrows()):\n",
    "    weight2.append(round(1/outlinks_per_node2[row[1][1]], 3))\n",
    "    \n",
    "df2['weight'] = weight2\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G2 = nx.from_pandas_edgelist(df2, \"source\", \"target\",create_using=nx.MultiDiGraph(), edge_attr = 'weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created the new graph we can implement on it the pagerank algorithm. The idea behind the pagerank algorithm based on the random-surfer is that, starting from a random page, a user will follow a direct link to another page with probability equal to the weight that we associated to each node, OR it will visit a random page (not following a link) with:       \n",
    "$$\\textrm{probability} = \\frac{1}{\\textrm{total number of nodes in the graph}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prova con le prime 1000 righe\n",
    "M = np.zeros((len(nodes),len(nodes)))\n",
    "n = len(M)\n",
    "\n",
    "for i,source in tqdm(enumerate(nodes)):\n",
    "    nodes_linked = [elem for elem in G2[source]]\n",
    "    \n",
    "    if len(nodes_linked)!=0:\n",
    "        for target in nodes_linked:\n",
    "            M[i][nodes.index(target)] = G2[source][target][0]['weight'] \n",
    "    \n",
    "    if round(sum(M[i]),1) == 0.0:\n",
    "        M[i] = 1/n "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = pagerank(100, 0.15, nodes, M)\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "for elem in v[0]:\n",
    "    c+=elem\n",
    "c   #perchè non fa 1?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
